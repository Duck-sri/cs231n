{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List,Optional,Dict,Tuple,Union,Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import Compose,PILToTensor,Normalize\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning And Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(Dataset):\n",
    "    \n",
    "    def __init__(self,data_dir:str,train:bool=True,transform=None,label_transform=None):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.Xs = np.array([])\n",
    "        self.ys = np.array([])\n",
    "        self.names = {}\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "        self.label_names = {  \n",
    "            0: \"airplane\",\n",
    "            1: \"automobile\",\n",
    "            2: \"bird\",\n",
    "            3: \"cat\",\n",
    "            4: \"deer\",\n",
    "            5: \"dog\",\n",
    "            6: \"frog\",\n",
    "            7: \"horse\",\n",
    "            8: \"ship\" ,\n",
    "            9: \"truck\",\n",
    "        }\n",
    "        \n",
    "        def filename_to_name_idx(name):\n",
    "            name = str(name).split('_')\n",
    "            return name[-1],name[0]\n",
    "        \n",
    "        if train:\n",
    "            files = filter(lambda file: 'data' in file, os.listdir(self.data_dir))\n",
    "        else:\n",
    "            files = filter(lambda file: 'test' in file, os.listdir(self.data_dir))\n",
    "\n",
    "        count = 0\n",
    "        for file in files:\n",
    "            data,labels,names = self.unpickle(os.path.join(self.data_dir,file))\n",
    "            if count == 0:\n",
    "                self.Xs = np.array(data)\n",
    "                self.ys = np.array(labels)\n",
    "                count += 1\n",
    "            else:\n",
    "                self.Xs = np.vstack((self.Xs,data))\n",
    "                self.ys = np.vstack((self.ys,labels))\n",
    "\n",
    "            names = {idx:name for idx,name in map(filename_to_name_idx,names)}\n",
    "\n",
    "            self.names.update(names)\n",
    "        \n",
    "        self.ys = self.ys.reshape(-1)\n",
    "        assert self.Xs.shape[0]==self.ys.shape[0],f\"Data and labels are not in same shape {self.Xs.shape,self.ys.shape}\"\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.Xs.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx) -> Tuple[np.ndarray,Union[int,np.ndarray]]:\n",
    "        image = self.Xs[idx]\n",
    "        label = self.ys[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label)\n",
    "        \n",
    "        return image,label\n",
    "    \n",
    "    def show_example(self,idx:int) -> None:\n",
    "        img,label = self.__getitem__(idx)\n",
    "        plt.imshow(img.transpose(1,2,0))\n",
    "        plt.title(self.label_names[label])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def show_random_example(self) -> None:\n",
    "        idx = np.random.randint(0,self.__len__())\n",
    "        self.show_example(idx)\n",
    "        \n",
    "    def get_random_grid(self,grid_size:int = 5,viz:bool=False) -> np.ndarray:\n",
    "        \n",
    "        idx = lambda : np.random.randint(0,len(self))\n",
    "        img = np.concatenate([ np.concatenate( [ self[idx()][0].transpose(1,2,0) for _ in range(grid_size)] ,axis=1) for _ in range(grid_size) ],axis=0)\n",
    "        if viz:\n",
    "            fig = plt.figure(figsize=(7,7))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        else:\n",
    "            return img\n",
    "            \n",
    "    @staticmethod\n",
    "    def unpickle(filename:str) -> Any:\n",
    "        with open(filename,'rb') as file:\n",
    "            batch = pickle.load(file,encoding='bytes')\n",
    "        \n",
    "        try:\n",
    "            labels = batch[b'labels']\n",
    "            data = batch[b'data'].reshape(-1,3,32,32)\n",
    "            names = batch[b'filenames']\n",
    "        except:\n",
    "            print(type(batch))\n",
    "        \n",
    "        return data,labels,names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/cifar-10-batches-py/')\n",
    "files = os.listdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Cifar10(data_dir,transform=lambda x: (x-x.mean())/x.std())\n",
    "ds_test = Cifar10(data_dir,train=False)\n",
    "\n",
    "split = 0.9\n",
    "train_set,val_set = torch.utils.data.random_split(\n",
    "    ds_train,\n",
    "    ( int(len(ds_train)*(split)) , int(len(ds_train)*(1-split)) + 1 )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bs = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=Bs,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=Bs,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=Bs,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    \"train\" : train_loader,\n",
    "    \"val\" : val_loader,\n",
    "    \"test\" : test_loader\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"train\" : train_set,\n",
    "    \"val\" : val_set,\n",
    "    \"test\" : ds_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 32, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forx.transpose(1,2).transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sigmoid sucks try ReLU, leakyReLU, ELU may see Tanh**\n",
    "\n",
    "1. Gradients tend to very small for higher values\n",
    "2. Gradients are zero-centric\n",
    "3. Exp is quite expensive to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actionvation functions\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def signmoid_d(z): return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def tanh(z): return np.tanh(z)\n",
    "def tanh_d(z): return (1-np.square(np.tanh(z)))\n",
    "\n",
    "def relu(z): return np.max(0,z)\n",
    "def relu_d(z): return np.where(z>0,1,0)\n",
    "\n",
    "def leakyRelu(z): return np.max(0.1*z,z)\n",
    "def leakyRelu_d(z): return np.where(z>0,1,-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
